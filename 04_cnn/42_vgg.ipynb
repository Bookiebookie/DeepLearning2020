{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfb9af38dcd38f730b7d3208490f4784",
     "grade": false,
     "grade_id": "cell-440df6cfa709812f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Number of points for this notebook:</b> 1\n",
    "<br>\n",
    "<b>Deadline:</b> March 23, 2020 (Monday). 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 4.2. Convolutional networks. VGG-style network.\n",
    "\n",
    "In the second part you need to train a convolutional neural network with an architecture inspired by a VGG-network [(Simonyan \\& Zisserman, 2015)](https://arxiv.org/abs/1409.1556)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65e2970339980ef7d85c3754662c4ee8",
     "grade": true,
     "grade_id": "evaluation_settings",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import tools\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is /coursedata\n"
     ]
    }
   ],
   "source": [
    "# When running on your own computer, you can specify the data directory by:\n",
    "# data_dir = tools.select_data_dir('/your/local/data/directory')\n",
    "data_dir = tools.select_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device for training (use GPU if you have one)\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48d33ffe246f5459117f53cac15b370d",
     "grade": false,
     "grade_id": "cell-fe95dcf02c6b9c5e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f2b11aa8f0d0377563333bd78493751",
     "grade": false,
     "grade_id": "cell-e5b565cc4aae8e7f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## FashionMNIST dataset\n",
    "\n",
    "Let us use the FashionMNIST dataset. It consists of 60,000 training images of 10 classes: 'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "889ef743f5d3f1f0499804525691113a",
     "grade": false,
     "grade_id": "cell-8b0fded08998282c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Transform to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Min-max scaling to [-1, 1]\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.FashionMNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST(root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal',\n",
    "           'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eccfe8c399d71b5018afc8ef3b0e9132",
     "grade": false,
     "grade_id": "cell-4ab9d042e4edcd54",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# VGG-style network\n",
    "\n",
    "Let us now define a convolution neural network with an architecture inspired by the [VGG-net](https://arxiv.org/abs/1409.1556):\n",
    "\n",
    "<img src=\"vgg-style.png\" width=600 style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3eb99ec6f8f808d272bdf9e3ca45be45",
     "grade": false,
     "grade_id": "cell-91295b0ab2098018",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The architecture:\n",
    "* A block of three convolutional layers with:\n",
    "    * 3x3 kernel\n",
    "    * 16 output channels\n",
    "    * one pixel zero-pading on both sides\n",
    "    * 2d batch normalization after each convolutional layer\n",
    "    * ReLU nonlinearity after each 2d batch normalization layer\n",
    "* Max pooling layer with 2x2 kernel and stride 2.\n",
    "* A block of three convolutional layers with:\n",
    "    * 3x3 kernel\n",
    "    * 32 output channels\n",
    "    * one pixel zero-pading on both sides\n",
    "    * 2d batch normalization after each convolutional layer\n",
    "    * ReLU nonlinearity after each 2d batch normalization layer\n",
    "* Max pooling layer with 2x2 kernel and stride 2.\n",
    "* One convolutional layer with:\n",
    "    * 3x3 kernel\n",
    "    * 48 output channels\n",
    "    * *no padding*\n",
    "    * 2d batch normalization after the convolutional layer\n",
    "    * ReLU nonlinearity after the 2d batch normalization layer\n",
    "* One convolutional layer with:\n",
    "    * 1x1 kernel\n",
    "    * 32 output channels\n",
    "    * *no padding*\n",
    "    * 2d batch normalization after the convolutional layer\n",
    "    * ReLU nonlinearity after the 2d batch normalization layer\n",
    "* One convolutional layer with:\n",
    "    * 1x1 kernel\n",
    "    * 16 output channels\n",
    "    * *no padding*\n",
    "    * 2d batch normalization after the convolutional layer\n",
    "    * ReLU nonlinearity after the 2d batch normalization layer\n",
    "* Global average pooling (compute the average value of each channel across all the input locations):\n",
    "    * 5x5 kernel (the input of the layer should be 5x5)\n",
    "* A fully-connected layer with 10 outputs (no nonlinearity)\n",
    "\n",
    "Notes:\n",
    "* Batch normalization is expected to be right after a convolutional layer, before nonlinearity.\n",
    "* We recommend that you check the number of modules with trainable parameters in your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b96cc788a6674bd8e1fe1e00473c4bd",
     "grade": false,
     "grade_id": "cell-958d9ce586a51bd3",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class VGGNet(nn.Module):\n",
    "    def __init__(self, n_channels=16):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          n_channels (int): Number of channels in the first convolutional layer. The number of channels in the following layers are the multipliers of n_channels. Hence, parameters of the layers to follow can be defined using this variable.\n",
    "        \"\"\"\n",
    "        super(VGGNet, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 48, kernel_size = 3),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(48, 32, kernel_size = 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, kernel_size = 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(16, 10)\n",
    "        #self.pool = nn.AvgPool2d(5*5,kernel_size = 5)\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (batch_size, 1, 28, 28): Input images.\n",
    "          verbose: True if you want to print the shapes of the intermediate variables.\n",
    "        \n",
    "        Returns:\n",
    "          y of shape (batch_size, 10): Outputs of the network.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        x = F.max_pool2d(self.layer1(x), kernel_size = 2, stride = 2)\n",
    "        x = F.max_pool2d(self.layer2(x), kernel_size = 2, stride = 2)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = F.avg_pool2d(x,5)\n",
    "        x = x.view(-1,16)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44870ad6dd02b719505a9dfd353943ac",
     "grade": false,
     "grade_id": "cell-e9e9a9dcda247c96",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input tensor: torch.Size([32, 1, 28, 28])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_VGGNet_shapes():\n",
    "    net = VGGNet()\n",
    "    net.to(device)\n",
    "\n",
    "    # Feed a batch of images from the training data to test the network\n",
    "    with torch.no_grad():\n",
    "        images, labels = iter(trainloader).next()\n",
    "        images = images.to(device)\n",
    "        print('Shape of the input tensor:', images.shape)\n",
    "\n",
    "        y = net(images, verbose=True)\n",
    "        assert y.shape == torch.Size([trainloader.batch_size, 10]), f\"Bad y.shape: {y.shape}\"\n",
    "\n",
    "    print('Success')\n",
    "\n",
    "test_VGGNet_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac8bd1127d6be9c8e72d608c8177aa5f",
     "grade": true,
     "grade_id": "test_VGGNet",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor([[ 8.0032,  8.0032,  8.0032,  8.0032,  8.0032, -8.0032, -8.0032, -8.0032,\n",
      "         -8.0032, -8.0032]], grad_fn=<AddmmBackward>)\n",
      "expected: tensor([ 8.0032,  8.0032,  8.0032,  8.0032,  8.0032, -8.0032, -8.0032, -8.0032,\n",
      "        -8.0032, -8.0032])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "tests.test_vgg_net(VGGNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c87ccb8eb839f919438ec33b6d8f5e3b",
     "grade": false,
     "grade_id": "cell-6c5c6ddc6d0312e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f83e3828402226248c665aa6b1f49a8",
     "grade": false,
     "grade_id": "cell-9e3a0480727aec61",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This function computes the accuracy on the test dataset\n",
    "def compute_accuracy(net, testloader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53272435c1d8e1491376543104def5b6",
     "grade": false,
     "grade_id": "cell-d168751b85ea2490",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training loop\n",
    "\n",
    "Your task is to implement the training loop. The recommended hyperparameters:\n",
    "* Adam optimizer with learning rate 0.01.\n",
    "* Cross-entropy loss. Note that we did not use softmax nonlinearity in the final layer of our network. Therefore, we need to use a loss function with log_softmax implemented, such as [`nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss).\n",
    "* Number of epochs: 10\n",
    "\n",
    "We recommend you to use function `compute_accuracy()` defined above to track the accaracy during training. The test accuracy should be above 0.87.\n",
    "\n",
    "**Note: function `compute_accuracy()` sets the network into the evaluation mode which changes the way the batch statistics are computed in batch normalization. You need to set the network into the training mode (by calling `net.train()`) when you want to perform training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db3a9615921b7a80d333475aa66ae69b",
     "grade": false,
     "grade_id": "cell-d5bf19391acb3661",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "net = VGGNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6270848f5387bf01aba9bb5f50303a78",
     "grade": false,
     "grade_id": "training_loop",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100]loss: 1.571\n",
      "[1, 200]loss: 0.844\n",
      "[1, 300]loss: 0.702\n",
      "[1, 400]loss: 0.597\n",
      "[1, 500]loss: 0.578\n",
      "[1, 600]loss: 0.497\n",
      "[1, 700]loss: 0.492\n",
      "[1, 800]loss: 0.479\n",
      "[1, 900]loss: 0.426\n",
      "[1, 1000]loss: 0.420\n",
      "[1, 1100]loss: 0.422\n",
      "[1, 1200]loss: 0.367\n",
      "[1, 1300]loss: 0.389\n",
      "[1, 1400]loss: 0.373\n",
      "[1, 1500]loss: 0.381\n",
      "[1, 1600]loss: 0.383\n",
      "[1, 1700]loss: 0.357\n",
      "[1, 1800]loss: 0.365\n",
      "[2, 100]loss: 0.322\n",
      "[2, 200]loss: 0.333\n",
      "[2, 300]loss: 0.314\n",
      "[2, 400]loss: 0.299\n",
      "[2, 500]loss: 0.334\n",
      "[2, 600]loss: 0.292\n",
      "[2, 700]loss: 0.297\n",
      "[2, 800]loss: 0.302\n",
      "[2, 900]loss: 0.306\n",
      "[2, 1000]loss: 0.305\n",
      "[2, 1100]loss: 0.303\n",
      "[2, 1200]loss: 0.298\n",
      "[2, 1300]loss: 0.283\n",
      "[2, 1400]loss: 0.290\n",
      "[2, 1500]loss: 0.282\n",
      "[2, 1600]loss: 0.261\n",
      "[2, 1700]loss: 0.284\n",
      "[2, 1800]loss: 0.295\n",
      "[3, 100]loss: 0.249\n",
      "[3, 200]loss: 0.261\n",
      "[3, 300]loss: 0.252\n",
      "[3, 400]loss: 0.238\n",
      "[3, 500]loss: 0.267\n",
      "[3, 600]loss: 0.257\n",
      "[3, 700]loss: 0.285\n",
      "[3, 800]loss: 0.248\n",
      "[3, 900]loss: 0.267\n",
      "[3, 1000]loss: 0.229\n",
      "[3, 1100]loss: 0.277\n",
      "[3, 1200]loss: 0.255\n",
      "[3, 1300]loss: 0.248\n",
      "[3, 1400]loss: 0.258\n",
      "[3, 1500]loss: 0.254\n",
      "[3, 1600]loss: 0.244\n",
      "[3, 1700]loss: 0.248\n",
      "[3, 1800]loss: 0.240\n",
      "[4, 100]loss: 0.219\n",
      "[4, 200]loss: 0.236\n",
      "[4, 300]loss: 0.242\n",
      "[4, 400]loss: 0.239\n",
      "[4, 500]loss: 0.233\n",
      "[4, 600]loss: 0.231\n",
      "[4, 700]loss: 0.218\n",
      "[4, 800]loss: 0.231\n",
      "[4, 900]loss: 0.252\n",
      "[4, 1000]loss: 0.213\n",
      "[4, 1100]loss: 0.223\n",
      "[4, 1200]loss: 0.247\n",
      "[4, 1300]loss: 0.222\n",
      "[4, 1400]loss: 0.220\n",
      "[4, 1500]loss: 0.254\n",
      "[4, 1600]loss: 0.227\n",
      "[4, 1700]loss: 0.217\n",
      "[4, 1800]loss: 0.244\n",
      "[5, 100]loss: 0.192\n",
      "[5, 200]loss: 0.217\n",
      "[5, 300]loss: 0.209\n",
      "[5, 400]loss: 0.215\n",
      "[5, 500]loss: 0.199\n",
      "[5, 600]loss: 0.193\n",
      "[5, 700]loss: 0.201\n",
      "[5, 800]loss: 0.197\n",
      "[5, 900]loss: 0.238\n",
      "[5, 1000]loss: 0.199\n",
      "[5, 1100]loss: 0.217\n",
      "[5, 1200]loss: 0.229\n",
      "[5, 1300]loss: 0.197\n",
      "[5, 1400]loss: 0.224\n",
      "[5, 1500]loss: 0.204\n",
      "[5, 1600]loss: 0.207\n",
      "[5, 1700]loss: 0.213\n",
      "[5, 1800]loss: 0.212\n",
      "[6, 100]loss: 0.198\n",
      "[6, 200]loss: 0.216\n",
      "[6, 300]loss: 0.209\n",
      "[6, 400]loss: 0.185\n",
      "[6, 500]loss: 0.180\n",
      "[6, 600]loss: 0.186\n",
      "[6, 700]loss: 0.178\n",
      "[6, 800]loss: 0.196\n",
      "[6, 900]loss: 0.186\n",
      "[6, 1000]loss: 0.200\n",
      "[6, 1100]loss: 0.179\n",
      "[6, 1200]loss: 0.201\n",
      "[6, 1300]loss: 0.211\n",
      "[6, 1400]loss: 0.195\n",
      "[6, 1500]loss: 0.187\n",
      "[6, 1600]loss: 0.208\n",
      "[6, 1700]loss: 0.199\n",
      "[6, 1800]loss: 0.197\n",
      "[7, 100]loss: 0.164\n",
      "[7, 200]loss: 0.186\n",
      "[7, 300]loss: 0.167\n",
      "[7, 400]loss: 0.193\n",
      "[7, 500]loss: 0.185\n",
      "[7, 600]loss: 0.180\n",
      "[7, 700]loss: 0.186\n",
      "[7, 800]loss: 0.173\n",
      "[7, 900]loss: 0.165\n",
      "[7, 1000]loss: 0.185\n",
      "[7, 1100]loss: 0.201\n",
      "[7, 1200]loss: 0.189\n",
      "[7, 1300]loss: 0.171\n",
      "[7, 1400]loss: 0.191\n",
      "[7, 1500]loss: 0.205\n",
      "[7, 1600]loss: 0.170\n",
      "[7, 1700]loss: 0.186\n",
      "[7, 1800]loss: 0.200\n",
      "[8, 100]loss: 0.183\n",
      "[8, 200]loss: 0.150\n",
      "[8, 300]loss: 0.174\n",
      "[8, 400]loss: 0.185\n",
      "[8, 500]loss: 0.174\n",
      "[8, 600]loss: 0.172\n",
      "[8, 700]loss: 0.166\n",
      "[8, 800]loss: 0.190\n",
      "[8, 900]loss: 0.171\n",
      "[8, 1000]loss: 0.202\n",
      "[8, 1100]loss: 0.165\n",
      "[8, 1200]loss: 0.146\n",
      "[8, 1300]loss: 0.159\n",
      "[8, 1400]loss: 0.156\n",
      "[8, 1500]loss: 0.172\n",
      "[8, 1600]loss: 0.172\n",
      "[8, 1700]loss: 0.173\n",
      "[8, 1800]loss: 0.185\n",
      "[9, 100]loss: 0.155\n",
      "[9, 200]loss: 0.150\n",
      "[9, 300]loss: 0.157\n",
      "[9, 400]loss: 0.181\n",
      "[9, 500]loss: 0.159\n",
      "[9, 600]loss: 0.151\n",
      "[9, 700]loss: 0.161\n",
      "[9, 800]loss: 0.156\n",
      "[9, 900]loss: 0.175\n",
      "[9, 1000]loss: 0.172\n",
      "[9, 1100]loss: 0.180\n",
      "[9, 1200]loss: 0.165\n",
      "[9, 1300]loss: 0.183\n",
      "[9, 1400]loss: 0.169\n",
      "[9, 1500]loss: 0.159\n",
      "[9, 1600]loss: 0.160\n",
      "[9, 1700]loss: 0.168\n",
      "[9, 1800]loss: 0.162\n",
      "[10, 100]loss: 0.129\n",
      "[10, 200]loss: 0.154\n",
      "[10, 300]loss: 0.145\n",
      "[10, 400]loss: 0.152\n",
      "[10, 500]loss: 0.147\n",
      "[10, 600]loss: 0.146\n",
      "[10, 700]loss: 0.171\n",
      "[10, 800]loss: 0.159\n",
      "[10, 900]loss: 0.154\n",
      "[10, 1000]loss: 0.168\n",
      "[10, 1100]loss: 0.152\n",
      "[10, 1200]loss: 0.146\n",
      "[10, 1300]loss: 0.169\n",
      "[10, 1400]loss: 0.150\n",
      "[10, 1500]loss: 0.161\n",
      "[10, 1600]loss: 0.156\n",
      "[10, 1700]loss: 0.146\n",
      "[10, 1800]loss: 0.167\n"
     ]
    }
   ],
   "source": [
    "# Implement the training loop in this cell\n",
    "LearningRate = 0.01\n",
    "Momentum = 0.9\n",
    "Epochs = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(),lr = LearningRate, momentum = Momentum)\n",
    "\n",
    "net.train()\n",
    "\n",
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    for epoch in range(Epochs):\n",
    "        loss_sum = 0.0\n",
    "        #read data form trainloader\n",
    "        for batch, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device),labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_sum += loss.item()\n",
    "            \n",
    "            if batch % 100 == 99:\n",
    "                print('[%d, %d]loss: %.03f' % (epoch + 1, batch+1, loss_sum/100))\n",
    "                loss_sum = 0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25f0ee95304fa8c33a23198227c373c7",
     "grade": false,
     "grade_id": "cell-28a73d35f35e73c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to save the model (type yes to confirm)? yes\n",
      "Model saved to 4_vgg_net.pth.\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "if not skip_training:\n",
    "    tools.save_model(net, '4_vgg_net.pth')\n",
    "else:\n",
    "    net = VGGNet()\n",
    "    tools.load_model(net, '4_vgg_net.pth', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f269c035c5fcabef3a8b90071c87f21",
     "grade": true,
     "grade_id": "accuracy",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the VGG net on the test images: 0.925\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy on the test set\n",
    "accuracy = compute_accuracy(net, testloader)\n",
    "print('Accuracy of the VGG net on the test images: %.3f' % accuracy)\n",
    "assert accuracy > 0.87, \"Poor accuracy ({:.3f})\".format(accuracy)\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
